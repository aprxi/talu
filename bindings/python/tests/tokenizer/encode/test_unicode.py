"""
Unicode and multilingual encoding tests.

Tests for talu.Tokenizer.encode() with Unicode text.
"""

import pytest


class TestEncodeUnicode:
    """Tests for Unicode text encoding."""

    @pytest.mark.requires_model
    @pytest.mark.parametrize(
        "text",
        [
            "Cafe resume naive",  # ASCII approximation
            "Cafe\u0301",  # With combining acute accent
        ],
    )
    def test_encode_unicode_basic(self, tokenizer, text):
        """Basic Unicode text encodes successfully."""
        tokens = tokenizer.encode(text)

        assert len(tokens) >= 1

    @pytest.mark.requires_model
    def test_encode_accented_chars(self, tokenizer):
        """Accented characters encode correctly."""
        accented = [
            "\u00e9",  # e-acute (precomposed)
            "e\u0301",  # e + combining acute (decomposed)
            "\u00f1",  # n-tilde
            "\u00fc",  # u-umlaut
        ]
        for char in accented:
            tokens = tokenizer.encode(char)
            assert len(tokens) >= 1

    @pytest.mark.requires_model
    def test_encode_unicode_normalization(self, tokenizer):
        """Different Unicode normalizations may tokenize differently."""
        # Precomposed vs decomposed
        precomposed = "\u00e9"  # e-acute as single codepoint
        decomposed = "e\u0301"  # e + combining acute

        tokens_pre = tokenizer.encode(precomposed)
        tokens_dec = tokenizer.encode(decomposed)

        # Both should encode (result may or may not be same)
        assert len(tokens_pre) >= 1
        assert len(tokens_dec) >= 1


class TestEncodeMultilingual:
    """Tests for multilingual text encoding."""

    @pytest.mark.requires_model
    def test_encode_japanese(self, tokenizer):
        """Japanese text encodes."""
        tokens = tokenizer.encode("æ—¥æœ¬èªžãƒ†ã‚¹ãƒˆ")

        assert len(tokens) >= 1

    @pytest.mark.requires_model
    def test_encode_chinese(self, tokenizer):
        """Chinese text encodes."""
        tokens = tokenizer.encode("ä¸­æ–‡æµ‹è¯•")

        assert len(tokens) >= 1

    @pytest.mark.requires_model
    def test_encode_korean(self, tokenizer):
        """Korean text encodes."""
        tokens = tokenizer.encode("í•œêµ­ì–´ í…ŒìŠ¤íŠ¸")

        assert len(tokens) >= 1

    @pytest.mark.requires_model
    def test_encode_russian(self, tokenizer):
        """Russian (Cyrillic) text encodes."""
        tokens = tokenizer.encode("ÐŸÑ€Ð¸Ð²ÐµÑ‚ Ð¼Ð¸Ñ€")

        assert len(tokens) >= 1

    @pytest.mark.requires_model
    def test_encode_arabic(self, tokenizer):
        """Arabic text encodes."""
        tokens = tokenizer.encode("Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…")

        assert len(tokens) >= 1

    @pytest.mark.requires_model
    def test_encode_french(self, tokenizer):
        """French text with accents encodes."""
        tokens = tokenizer.encode("Bonjour le monde")

        assert len(tokens) >= 1

    @pytest.mark.requires_model
    def test_encode_german(self, tokenizer):
        """German text with umlauts encodes."""
        tokens = tokenizer.encode("Hallo Welt")

        assert len(tokens) >= 1

    @pytest.mark.requires_model
    @pytest.mark.parametrize(
        "lang,text",
        [
            ("japanese", "æ—¥æœ¬èªžãƒ†ã‚¹ãƒˆ"),
            ("chinese", "ä¸­æ–‡æµ‹è¯•"),
            ("korean", "í•œêµ­ì–´ í…ŒìŠ¤íŠ¸"),
            ("russian", "ÐŸÑ€Ð¸Ð²ÐµÑ‚ Ð¼Ð¸Ñ€"),
            ("arabic", "Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…"),
            ("french", "Bonjour le monde"),
            ("german", "Hallo Welt"),
            ("italian", "Ciao mondo"),
        ],
    )
    def test_encode_multilingual_parametrized(self, tokenizer, lang, text):
        """Parametrized multilingual encoding test."""
        tokens = tokenizer.encode(text)

        assert len(tokens) >= 1, f"Failed for {lang}: {text}"


class TestEncodeEmoji:
    """Tests for emoji encoding."""

    @pytest.mark.requires_model
    def test_encode_simple_emoji(self, tokenizer):
        """Simple emoji encodes."""
        tokens = tokenizer.encode("ðŸŽ‰")

        assert len(tokens) >= 1

    @pytest.mark.requires_model
    def test_encode_emoji_with_text(self, tokenizer):
        """Emoji with text encodes."""
        tokens = tokenizer.encode("ðŸŽ‰ Emoji test ðŸš€")

        assert len(tokens) >= 1

    @pytest.mark.requires_model
    @pytest.mark.parametrize(
        "emoji",
        [
            "ðŸ˜€",  # Simple emoji
            "ðŸ‘¨â€ðŸ‘©â€ðŸ‘§â€ðŸ‘¦",  # Family emoji (ZWJ sequence)
            "ðŸ‡ºðŸ‡¸",  # Flag emoji
            "ðŸ‘ðŸ½",  # Emoji with skin tone modifier
        ],
    )
    def test_encode_various_emoji(self, tokenizer, emoji):
        """Various emoji types encode."""
        tokens = tokenizer.encode(emoji)

        assert len(tokens) >= 1


class TestEncodeMixedScripts:
    """Tests for mixed script encoding."""

    @pytest.mark.requires_model
    def test_encode_mixed_cjk_latin(self, tokenizer):
        """Mixed CJK and Latin text encodes."""
        tokens = tokenizer.encode("Hello ä¸–ç•Œ!")

        assert len(tokens) >= 1

    @pytest.mark.requires_model
    def test_encode_mixed_with_numbers(self, tokenizer):
        """Mixed scripts with numbers encode."""
        tokens = tokenizer.encode("Testing 123 æ—¥æœ¬èªž")

        assert len(tokens) >= 1

    @pytest.mark.requires_model
    @pytest.mark.parametrize(
        "text",
        [
            "Hello ä¸–ç•Œ!",
            "Testing 123 æ—¥æœ¬èªž",
            "ÐŸÑ€Ð¸Ð²ÐµÑ‚ Hello ä½ å¥½",
            "ðŸŽ‰ Party! ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ¼ æ´¾å°",
        ],
    )
    def test_encode_mixed_scripts_parametrized(self, tokenizer, text):
        """Parametrized mixed script tests."""
        tokens = tokenizer.encode(text)

        assert len(tokens) >= 1
