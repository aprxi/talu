//! Tokenizer fixture tests driven by an external dataset.
//!
//! Validates encode, decode-roundtrip, vocabulary decode, and chat template
//! rendering against expected outputs generated by a reference tokenizer
//! (HuggingFace transformers).
//!
//! Opt-in via `TALU_TOKENIZER_FIXTURES` pointing at the dataset root.
//! Silently skipped when the env var is absent.

use regex::Regex;
use serde::Deserialize;
use std::collections::BTreeMap;
use std::ffi::{c_char, c_int, c_void, CStr, CString};
use std::path::{Path, PathBuf};
use std::sync::LazyLock;
use std::{collections::HashMap, fs, ptr};

/// Max example failures to keep per model (keeps output readable).
const MAX_EXAMPLES: usize = 3;

/// Regex patterns to normalize dynamic dates in chat template output.
/// Must match the Python script's `normalize_dates()`.
static DATE_NORMALIZERS: LazyLock<Vec<(Regex, &'static str)>> = LazyLock::new(|| {
    vec![
        // "Today Date: 13 Feb 2026" or "Today's Date: 13 Feb 2026"
        (
            Regex::new(
                r"(Today(?:'s)?\s+Date:\s*)\d{1,2} (?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) \d{4}"
            ).unwrap(),
            "${1}[TODAY]",
        ),
        // "Today's Date: February 13, 2026"
        (
            Regex::new(
                r"(Today(?:'s)?\s+Date:\s*)(?:January|February|March|April|May|June|July|August|September|October|November|December) \d{1,2},?\s*\d{4}"
            ).unwrap(),
            "${1}[TODAY]",
        ),
        // "Current date: 2026-02-13"
        (
            Regex::new(r"(Current date:\s*)\d{4}-\d{2}-\d{2}").unwrap(),
            "${1}[TODAY]",
        ),
    ]
});

fn normalize_dates(text: &str) -> String {
    let mut result = text.to_string();
    for (re, replacement) in DATE_NORMALIZERS.iter() {
        result = re.replace_all(&result, *replacement).to_string();
    }
    result
}

// ---------------------------------------------------------------------------
// Correctly-typed FFI declarations
// ---------------------------------------------------------------------------

extern "C" {
    fn talu_tokenizer_encode(
        handle: *mut c_void,
        text: *const u8,
        text_len: usize,
        options: *const talu_sys::EncodeOptions,
    ) -> talu_sys::EncodeResult;

    fn talu_tokenizer_decode(
        handle: *mut c_void,
        tokens: *const u32,
        num_tokens: usize,
        options: *const talu_sys::DecodeOptionsC,
    ) -> talu_sys::DecodeResult;
}

// ---------------------------------------------------------------------------
// Dataset schema
// ---------------------------------------------------------------------------

#[derive(Deserialize)]
struct Expected {
    encode: Vec<EncodeEntry>,
    roundtrip: Vec<RoundtripEntry>,
    chat_template: Vec<ChatTemplateEntry>,
}

#[derive(Deserialize)]
struct EncodeEntry {
    input: String,
    ids: Option<Vec<u32>>,
    ids_special: Option<Vec<u32>>,
    /// Present when the reference tokenizer failed; skip this entry.
    error: Option<String>,
    error_special: Option<String>,
}

#[derive(Deserialize)]
struct RoundtripEntry {
    input: String,
    decoded: Option<String>,
    stable: Option<bool>,
    error: Option<String>,
}

#[derive(Deserialize)]
struct ChatTemplateEntry {
    name: String,
    messages: Vec<serde_json::Value>,
    add_generation_prompt: bool,
    expected: Option<String>,
    /// Present when the reference tokenizer failed; skip this entry.
    error: Option<String>,
}

#[derive(Deserialize)]
struct Meta {
    bos_token: Option<String>,
    eos_token: Option<String>,
}


// ---------------------------------------------------------------------------
// Tokenizer handle (loads from JSON bytes)
// ---------------------------------------------------------------------------

struct FixtureTokenizer {
    handle: *mut c_void,
}

impl FixtureTokenizer {
    /// Try to create a tokenizer from JSON bytes. Returns `None` if the
    /// tokenizer format is unsupported by the current build.
    fn try_from_json(json: &[u8]) -> Option<Self> {
        let mut handle: *mut c_void = ptr::null_mut();
        let rc = unsafe {
            talu_sys::talu_tokenizer_create_from_json(
                json.as_ptr(),
                json.len(),
                &mut handle as *mut _ as *mut c_void,
            )
        };
        if rc != 0 || handle.is_null() {
            return None;
        }
        Some(Self { handle })
    }

    fn encode(&self, text: &str) -> Result<Vec<u32>, String> {
        let opts = talu_sys::EncodeOptions::default();
        let result = unsafe {
            talu_tokenizer_encode(
                self.handle,
                text.as_bytes().as_ptr(),
                text.len(),
                &opts,
            )
        };
        if !result.error_msg.is_null() {
            let msg = unsafe { CStr::from_ptr(result.error_msg as *const c_char) }
                .to_string_lossy()
                .to_string();
            return Err(msg);
        }
        let tokens = if result.ids.is_null() || result.num_tokens == 0 {
            Vec::new()
        } else {
            unsafe { std::slice::from_raw_parts(result.ids, result.num_tokens) }.to_vec()
        };
        unsafe { talu_sys::talu_encode_result_free(result) };
        Ok(tokens)
    }

    fn encode_special(&self, text: &str) -> Result<Vec<u32>, String> {
        let opts = talu_sys::EncodeOptions { add_bos: 1, ..Default::default() };
        let result = unsafe {
            talu_tokenizer_encode(
                self.handle,
                text.as_bytes().as_ptr(),
                text.len(),
                &opts,
            )
        };
        if !result.error_msg.is_null() {
            let msg = unsafe { CStr::from_ptr(result.error_msg as *const c_char) }
                .to_string_lossy()
                .to_string();
            return Err(msg);
        }
        let tokens = if result.ids.is_null() || result.num_tokens == 0 {
            Vec::new()
        } else {
            unsafe { std::slice::from_raw_parts(result.ids, result.num_tokens) }.to_vec()
        };
        unsafe { talu_sys::talu_encode_result_free(result) };
        Ok(tokens)
    }

    fn decode(&self, tokens: &[u32]) -> Result<String, String> {
        let opts = talu_sys::DecodeOptionsC { skip_special_tokens: 0 };
        let result = unsafe {
            talu_tokenizer_decode(
                self.handle,
                tokens.as_ptr(),
                tokens.len(),
                &opts,
            )
        };
        if !result.error_msg.is_null() {
            let msg = unsafe { CStr::from_ptr(result.error_msg as *const c_char) }
                .to_string_lossy()
                .to_string();
            return Err(msg);
        }
        if result.text.is_null() || result.text_len == 0 {
            let s = String::new();
            unsafe { talu_sys::talu_decode_result_free(result.text, result.text_len) };
            return Ok(s);
        }
        let text = unsafe {
            let slice = std::slice::from_raw_parts(result.text, result.text_len);
            String::from_utf8_lossy(slice).to_string()
        };
        unsafe { talu_sys::talu_decode_result_free(result.text, result.text_len) };
        Ok(text)
    }
}

impl Drop for FixtureTokenizer {
    fn drop(&mut self) {
        unsafe { talu_sys::talu_tokenizer_free(self.handle) };
    }
}

// ---------------------------------------------------------------------------
// Chat template helper
// ---------------------------------------------------------------------------

fn apply_chat_template(
    template: &str,
    messages_json: &str,
    add_generation_prompt: bool,
    bos_token: &str,
    eos_token: &str,
) -> Result<String, i32> {
    let msgs = CString::new(messages_json).unwrap();
    let bos = CString::new(bos_token).unwrap();
    let eos = CString::new(eos_token).unwrap();
    let mut out: *mut c_char = ptr::null_mut();

    let rc = unsafe {
        talu_sys::talu_apply_chat_template_string(
            template.as_ptr(),
            template.len(),
            msgs.as_ptr(),
            add_generation_prompt as c_int,
            bos.as_ptr(),
            eos.as_ptr(),
            &mut out as *mut _ as *mut c_void,
        )
    };

    if rc != 0 {
        return Err(rc);
    }

    assert!(!out.is_null(), "chat template succeeded but output is null");
    let s = unsafe { CStr::from_ptr(out) }
        .to_string_lossy()
        .to_string();
    unsafe { talu_sys::talu_text_free(out) };
    Ok(s)
}

/// Extract the chat template string from tokenizer_config.json.
///
/// The `chat_template` field can be a string or a list of
/// `{"name": "...", "template": "..."}` objects.
fn read_chat_template(model_path: &Path) -> Option<String> {
    let config_path = model_path.join("tokenizer_config.json");
    let content = fs::read_to_string(&config_path).ok()?;
    let config: serde_json::Value = serde_json::from_str(&content).ok()?;
    let ct = config.get("chat_template")?;

    match ct {
        serde_json::Value::String(s) => Some(s.clone()),
        serde_json::Value::Array(arr) => {
            // Use the "default" template, or the first one.
            for entry in arr {
                if entry.get("name").and_then(|n| n.as_str()) == Some("default") {
                    if let Some(t) = entry.get("template").and_then(|t| t.as_str()) {
                        return Some(t.to_string());
                    }
                }
            }
            arr.first()
                .and_then(|e| e.get("template"))
                .and_then(|t| t.as_str())
                .map(|s| s.to_string())
        }
        _ => None,
    }
}

// ---------------------------------------------------------------------------
// Discovery
// ---------------------------------------------------------------------------

fn fixtures_root() -> Option<PathBuf> {
    std::env::var("TALU_TOKENIZER_FIXTURES")
        .ok()
        .map(PathBuf::from)
}

fn discover_models(root: &Path) -> Vec<(String, PathBuf)> {
    let models_dir = root.join("models");
    let mut models = Vec::new();
    if let Ok(entries) = fs::read_dir(&models_dir) {
        for entry in entries.flatten() {
            let path = entry.path();
            if path.is_dir() && path.join("tokenizer.json").is_file() {
                let name = entry.file_name().to_string_lossy().to_string();
                models.push((name, path));
            }
        }
    }
    models.sort_by(|a, b| a.0.cmp(&b.0));
    models
}

/// Load a model's tokenizer and expected data.
/// Returns Err(message) if the tokenizer fails to load.
fn load_model(model_path: &Path) -> Result<(FixtureTokenizer, Expected), String> {
    let json_bytes = fs::read(model_path.join("tokenizer.json")).unwrap();
    let tok = FixtureTokenizer::try_from_json(&json_bytes)
        .ok_or_else(|| "talu_tokenizer_create_from_json returned error".to_string())?;
    let expected: Expected = serde_json::from_str(
        &fs::read_to_string(model_path.join("expected.json")).unwrap(),
    )
    .unwrap();

    Ok((tok, expected))
}

// ---------------------------------------------------------------------------
// Per-model failure tracking
// ---------------------------------------------------------------------------

struct ModelStats {
    passed: usize,
    failed: usize,
    examples: Vec<String>,
}

impl ModelStats {
    fn new() -> Self {
        Self { passed: 0, failed: 0, examples: Vec::new() }
    }

    fn pass(&mut self) {
        self.passed += 1;
    }

    fn fail(&mut self, detail: String) {
        self.failed += 1;
        if self.examples.len() < MAX_EXAMPLES {
            self.examples.push(detail);
        }
    }
}

/// Format a summary table + limited examples, return (total_pass, total_fail, message).
fn format_report(label: &str, stats: &BTreeMap<String, ModelStats>) -> (usize, usize, String) {
    let total_pass: usize = stats.values().map(|s| s.passed).sum();
    let total_fail: usize = stats.values().map(|s| s.failed).sum();
    let total_models = stats.len();

    let failing: Vec<_> = stats.iter().filter(|(_, s)| s.failed > 0).collect();

    let mut out = format!(
        "{label}: {total_fail} failures, {} passed out of {} tests across {total_models} models",
        total_pass,
        total_pass + total_fail,
    );

    if failing.is_empty() {
        return (total_pass, total_fail, out);
    }

    out.push_str(&format!("\n\n{} failing models:\n", failing.len()));
    for (name, s) in &failing {
        out.push_str(&format!(
            "  {name:60} {}/{} failed\n",
            s.failed,
            s.passed + s.failed,
        ));
    }

    out.push_str("\nExamples (first 3 per model):\n");
    for (name, s) in &failing {
        out.push_str(&format!("\n  --- {name} ({} failures) ---\n", s.failed));
        for ex in &s.examples {
            out.push_str(&format!("  {}\n", ex.replace('\n', "\n  ")));
        }
    }

    (total_pass, total_fail, out)
}

// ---------------------------------------------------------------------------
// Tests
// ---------------------------------------------------------------------------

/// Dimension 1: text -> token IDs must match reference.
#[test]
fn fixture_encode() {
    let root = match fixtures_root() {
        Some(r) => r,
        None => return,
    };
    let models = discover_models(&root);
    assert!(!models.is_empty(), "no models found in {}", root.display());

    let mut stats: BTreeMap<String, ModelStats> = BTreeMap::new();

    for (model_name, model_path) in &models {
        let s = stats.entry(model_name.clone()).or_insert_with(ModelStats::new);

        let (tok, expected) = match load_model(model_path) {
            Ok(v) => v,
            Err(e) => {
                s.fail(format!("load failed: {e}"));
                continue;
            }
        };

        for entry in &expected.encode {
            if entry.error.is_some() {
                continue;
            }
            let expected_ids = entry.ids.as_ref().unwrap();
            let actual = match tok.encode(&entry.input) {
                Ok(ids) => ids,
                Err(e) => {
                    s.fail(format!("encode({:?}) error: {e}", entry.input));
                    continue;
                }
            };
            if actual != *expected_ids {
                s.fail(format!(
                    "encode({:?})\n  expected: {:?}\n  actual:   {:?}",
                    entry.input, expected_ids, actual
                ));
            } else {
                s.pass();
            }
        }
    }

    let (_, total_fail, report) = format_report("fixture_encode", &stats);
    if total_fail > 0 {
        panic!("\n{report}");
    }
}

/// Dimension 5: text -> token IDs with special tokens (post-processor).
#[test]
fn fixture_encode_special() {
    let root = match fixtures_root() {
        Some(r) => r,
        None => return,
    };
    let models = discover_models(&root);
    assert!(!models.is_empty(), "no models found in {}", root.display());

    let mut stats: BTreeMap<String, ModelStats> = BTreeMap::new();

    for (model_name, model_path) in &models {
        let s = stats.entry(model_name.clone()).or_insert_with(ModelStats::new);

        let (tok, expected) = match load_model(model_path) {
            Ok(v) => v,
            Err(e) => {
                s.fail(format!("load failed: {e}"));
                continue;
            }
        };

        for entry in &expected.encode {
            if entry.error_special.is_some() {
                continue;
            }
            let expected_ids = match entry.ids_special.as_ref() {
                Some(ids) => ids,
                None => continue,
            };
            let actual = match tok.encode_special(&entry.input) {
                Ok(ids) => ids,
                Err(e) => {
                    s.fail(format!("encode_special({:?}) error: {e}", entry.input));
                    continue;
                }
            };
            if actual != *expected_ids {
                s.fail(format!(
                    "encode_special({:?})\n  expected: {:?}\n  actual:   {:?}",
                    entry.input, expected_ids, actual
                ));
            } else {
                s.pass();
            }
        }
    }

    let (_, total_fail, report) = format_report("fixture_encode_special", &stats);
    if total_fail > 0 {
        panic!("\n{report}");
    }
}

/// Dimension 3: encode -> decode roundtrip fidelity.
#[test]
fn fixture_roundtrip() {
    let root = match fixtures_root() {
        Some(r) => r,
        None => return,
    };
    let models = discover_models(&root);
    assert!(!models.is_empty());

    let mut stats: BTreeMap<String, ModelStats> = BTreeMap::new();

    for (model_name, model_path) in &models {
        let s = stats.entry(model_name.clone()).or_insert_with(ModelStats::new);

        let (tok, expected) = match load_model(model_path) {
            Ok(v) => v,
            Err(e) => {
                s.fail(format!("load failed: {e}"));
                continue;
            }
        };

        for entry in &expected.roundtrip {
            if entry.error.is_some() {
                continue;
            }
            let expected_decoded = entry.decoded.as_ref().unwrap();
            let stable = entry.stable.unwrap();

            let tokens = match tok.encode(&entry.input) {
                Ok(ids) => ids,
                Err(e) => {
                    s.fail(format!("roundtrip encode({:?}) error: {e}", entry.input));
                    continue;
                }
            };
            let decoded = match tok.decode(&tokens) {
                Ok(s_) => s_,
                Err(e) => {
                    s.fail(format!("roundtrip decode({:?}) error: {e}", entry.input));
                    continue;
                }
            };

            if decoded != *expected_decoded {
                s.fail(format!(
                    "roundtrip({:?})\n  expected decoded: {:?}\n  actual decoded:   {:?}",
                    entry.input, expected_decoded, decoded
                ));
                continue;
            }

            if stable {
                let re_tokens = match tok.encode(&decoded) {
                    Ok(ids) => ids,
                    Err(e) => {
                        s.fail(format!("roundtrip re-encode({:?}) error: {e}", entry.input));
                        continue;
                    }
                };
                if re_tokens != tokens {
                    s.fail(format!(
                        "roundtrip stability({:?})\n  first encode: {:?}\n  re-encode:    {:?}",
                        entry.input, tokens, re_tokens
                    ));
                    continue;
                }
            }
            s.pass();
        }
    }

    let (_, total_fail, report) = format_report("fixture_roundtrip", &stats);
    if total_fail > 0 {
        panic!("\n{report}");
    }
}

/// Dimension 2: decode every token ID in the vocabulary.
#[test]
fn fixture_vocab_decode() {
    let root = match fixtures_root() {
        Some(r) => r,
        None => return,
    };
    let models = discover_models(&root);
    assert!(!models.is_empty());

    let mut stats: BTreeMap<String, ModelStats> = BTreeMap::new();

    for (model_name, model_path) in &models {
        let vocab_path = model_path.join("vocab_decode.json");
        if !vocab_path.is_file() {
            continue;
        }

        let s = stats.entry(model_name.clone()).or_insert_with(ModelStats::new);

        let json_bytes = fs::read(model_path.join("tokenizer.json")).unwrap();
        let tok = match FixtureTokenizer::try_from_json(&json_bytes) {
            Some(t) => t,
            None => {
                s.fail("load failed: talu_tokenizer_create_from_json returned error".to_string());
                continue;
            }
        };

        let vocab: HashMap<String, String> =
            serde_json::from_str(&fs::read_to_string(&vocab_path).unwrap()).unwrap();

        for (id_str, expected_text) in &vocab {
            let id: u32 = id_str.parse().unwrap();
            let actual = match tok.decode(&[id]) {
                Ok(s_) => s_,
                Err(e) => {
                    s.fail(format!("decode([{id}]) error: {e}"));
                    continue;
                }
            };
            if actual != *expected_text {
                s.fail(format!(
                    "decode([{id}])\n  expected: {:?}\n  actual:   {:?}",
                    expected_text, actual
                ));
            } else {
                s.pass();
            }
        }
    }

    let (_, total_fail, report) = format_report("fixture_vocab_decode", &stats);
    if total_fail > 0 {
        panic!("\n{report}");
    }
}

/// Dimension 4: chat template rendering.
#[test]
fn fixture_chat_template() {
    let root = match fixtures_root() {
        Some(r) => r,
        None => return,
    };
    let models = discover_models(&root);
    assert!(!models.is_empty());

    let mut stats: BTreeMap<String, ModelStats> = BTreeMap::new();

    for (model_name, model_path) in &models {
        let expected: Expected = serde_json::from_str(
            &fs::read_to_string(model_path.join("expected.json")).unwrap(),
        )
        .unwrap();

        if expected.chat_template.is_empty() {
            continue;
        }

        let template = match read_chat_template(model_path) {
            Some(t) => t,
            None => continue,
        };

        let s = stats.entry(model_name.clone()).or_insert_with(ModelStats::new);

        let meta: Meta = serde_json::from_str(
            &fs::read_to_string(model_path.join("meta.json")).unwrap(),
        )
        .unwrap();
        let bos = meta.bos_token.as_deref().unwrap_or("");
        let eos = meta.eos_token.as_deref().unwrap_or("");

        for entry in &expected.chat_template {
            if entry.error.is_some() {
                continue;
            }
            let expected_text = entry.expected.as_ref().unwrap();
            let messages_json = serde_json::to_string(&entry.messages).unwrap();

            match apply_chat_template(
                &template,
                &messages_json,
                entry.add_generation_prompt,
                bos,
                eos,
            ) {
                Ok(actual) if normalize_dates(&actual) != *expected_text => {
                    s.fail(format!(
                        "chat_template({:?})\n  expected: {:?}\n  actual:   {:?}",
                        entry.name, expected_text, normalize_dates(&actual)
                    ));
                }
                Err(rc) => {
                    s.fail(format!(
                        "chat_template({:?}) returned error {rc}",
                        entry.name
                    ));
                }
                _ => { s.pass(); }
            }
        }
    }

    let (_, total_fail, report) = format_report("fixture_chat_template", &stats);
    if total_fail > 0 {
        panic!("\n{report}");
    }
}
