//! Multi-byte UTF-8 and non-ASCII input tests.
//!
//! The minimal fixture has a ByteLevel pre-tokenizer and a 99-token ASCII vocab.
//! Non-ASCII bytes map to characters outside the vocab â†’ `<unk>=3`.
//! Each input byte produces exactly one token (no merges in base fixture).

use crate::capi::tokenizer::common::TokenizerTestContext;

fn no_bos() -> talu_sys::EncodeOptions {
    talu_sys::EncodeOptions {
        add_bos: 0,
        ..Default::default()
    }
}

// ===========================================================================
// Encode: multi-byte UTF-8
// ===========================================================================

/// Accented Latin: "cafÃ©" â†’ 5 bytes (Ã© = 2 UTF-8 bytes), both non-ASCII â†’ unk.
#[test]
fn encode_accented_latin() {
    let ctx = TokenizerTestContext::new();
    // c=71, a=69, f=74, 0xC3â†’unk, 0xA9â†’unk
    assert_eq!(ctx.encode_with("cafÃ©", &no_bos()), [71, 69, 74, 3, 3]);
}

/// Single CJK character: "æ—¥" = 3 UTF-8 bytes, all â†’ unk.
#[test]
fn encode_cjk_single() {
    let ctx = TokenizerTestContext::new();
    assert_eq!(ctx.encode_with("æ—¥", &no_bos()), [3, 3, 3]);
}

/// Two CJK characters: "æ—¥æœ¬" = 6 UTF-8 bytes, all â†’ unk.
#[test]
fn encode_cjk_two_chars() {
    let ctx = TokenizerTestContext::new();
    assert_eq!(ctx.encode_with("æ—¥æœ¬", &no_bos()), [3, 3, 3, 3, 3, 3]);
}

/// Single emoji: "ğŸ‰" = 4 UTF-8 bytes, all â†’ unk.
#[test]
fn encode_emoji_4byte() {
    let ctx = TokenizerTestContext::new();
    assert_eq!(ctx.encode_with("ğŸ‰", &no_bos()), [3, 3, 3, 3]);
}

/// Mixed ASCII + emoji: "HiğŸ‰" â†’ ASCII tokens + 4 unk.
#[test]
fn encode_mixed_ascii_emoji() {
    let ctx = TokenizerTestContext::new();
    // H=44, i=77, then 4 emoji bytes â†’ unk
    assert_eq!(
        ctx.encode_with("HiğŸ‰", &no_bos()),
        [44, 77, 3, 3, 3, 3]
    );
}

/// Korean: "í•œ" = 3 UTF-8 bytes â†’ [3, 3, 3].
#[test]
fn encode_korean() {
    let ctx = TokenizerTestContext::new();
    assert_eq!(ctx.encode_with("í•œ", &no_bos()), [3, 3, 3]);
}

/// Russian: "Ğ–" = 2 UTF-8 bytes â†’ [3, 3].
#[test]
fn encode_cyrillic() {
    let ctx = TokenizerTestContext::new();
    assert_eq!(ctx.encode_with("Ğ–", &no_bos()), [3, 3]);
}

/// Arabic: "Ù…" = 2 UTF-8 bytes â†’ [3, 3].
#[test]
fn encode_arabic() {
    let ctx = TokenizerTestContext::new();
    assert_eq!(ctx.encode_with("Ù…", &no_bos()), [3, 3]);
}

/// ZWJ emoji sequence: "ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦" = 25 UTF-8 bytes, all â†’ unk.
#[test]
fn encode_emoji_zwj_sequence() {
    let ctx = TokenizerTestContext::new();
    let text = "ğŸ‘¨\u{200D}ğŸ‘©\u{200D}ğŸ‘§\u{200D}ğŸ‘¦"; // family ZWJ sequence
    assert_eq!(text.len(), 25);
    let tokens = ctx.encode_with(text, &no_bos());
    assert_eq!(tokens.len(), 25);
    assert!(tokens.iter().all(|&t| t == 3), "all bytes should be unk");
}

/// Flag emoji: "ğŸ‡ºğŸ‡¸" = 8 UTF-8 bytes, all â†’ unk.
#[test]
fn encode_flag_emoji() {
    let ctx = TokenizerTestContext::new();
    let text = "ğŸ‡ºğŸ‡¸";
    assert_eq!(text.len(), 8);
    assert_eq!(ctx.encode_with(text, &no_bos()), [3, 3, 3, 3, 3, 3, 3, 3]);
}

/// Skin-tone emoji: "ğŸ‘ğŸ½" = 8 UTF-8 bytes, all â†’ unk.
#[test]
fn encode_skin_tone_emoji() {
    let ctx = TokenizerTestContext::new();
    let text = "ğŸ‘ğŸ½";
    assert_eq!(text.len(), 8);
    assert_eq!(ctx.encode_with(text, &no_bos()), [3, 3, 3, 3, 3, 3, 3, 3]);
}

/// Mixed scripts: ASCII + CJK + Cyrillic in one string.
#[test]
fn encode_mixed_scripts() {
    let ctx = TokenizerTestContext::new();
    // "Hiæ—¥" = H(44), i(77), then 3 CJK bytes â†’ unk
    assert_eq!(ctx.encode_with("Hiæ—¥", &no_bos()), [44, 77, 3, 3, 3]);

    // "aĞ–b" = a(69), then 2 Cyrillic bytes â†’ unk, b(70)
    assert_eq!(ctx.encode_with("aĞ–b", &no_bos()), [69, 3, 3, 70]);
}

/// Token count equals byte count for any input (ByteLevel, no merges).
#[test]
fn token_count_equals_byte_count() {
    let ctx = TokenizerTestContext::new();
    let opts = no_bos();

    let cases = [
        "Hello",       // 5 bytes
        "cafÃ©",        // 5 bytes
        "æ—¥æœ¬èª",      // 9 bytes
        "ğŸ‰ğŸš€",       // 8 bytes
        "Hi ğŸ‰ bye",  // 10 bytes
    ];
    for text in cases {
        let tokens = ctx.encode_with(text, &opts);
        assert_eq!(
            tokens.len(),
            text.len(), // &str.len() is byte count in Rust
            "token count should equal byte count for {:?}",
            text
        );
    }
}

// ===========================================================================
// Decode roundtrip
// ===========================================================================

/// ASCII-only text roundtrips exactly through encodeâ†’decode.
#[test]
fn decode_roundtrip_ascii() {
    let ctx = TokenizerTestContext::new();
    let opts = no_bos();

    for text in ["Hello", "abc", "012", "!@#"] {
        let decoded = ctx.decode(&ctx.encode_with(text, &opts));
        assert_eq!(decoded, text, "roundtrip failed for {text:?}");
    }
}

/// Mixed text with unk tokens: decode produces unk substitution for non-ASCII.
/// The <unk> token string replaces each non-ASCII byte.
#[test]
fn decode_mixed_ascii_unk() {
    let ctx = TokenizerTestContext::new();
    let tokens = ctx.encode_with("cafÃ©", &no_bos());
    let decoded = ctx.decode(&tokens);
    // First 3 chars decode to "caf", last 2 tokens are <unk> substitutions.
    assert!(decoded.starts_with("caf"), "should start with ASCII portion");
    assert!(decoded.len() > 3, "should have unk substitutions after 'caf'");
}

// ===========================================================================
// Tokenize (string representation) with multi-byte input
// ===========================================================================

/// tokenize "cafÃ©" returns 5 string tokens (one per byte).
#[test]
fn tokenize_strings_multibyte() {
    let ctx = TokenizerTestContext::new();
    let text = "cafÃ©"; // 5 UTF-8 bytes
    let result = unsafe {
        talu_sys::talu_tokenizer_tokenize(
            ctx.handle(),
            text.as_bytes().as_ptr(),
            text.len(),
        )
    };
    assert!(result.error_msg.is_null());
    assert_eq!(result.num_tokens, 5);

    let ptrs = unsafe {
        std::slice::from_raw_parts(result.tokens as *const *const i8, result.num_tokens)
    };
    let tokens: Vec<String> = (0..result.num_tokens)
        .map(|i| {
            unsafe { std::ffi::CStr::from_ptr(ptrs[i]) }
                .to_string_lossy()
                .to_string()
        })
        .collect();
    // First 3 are ASCII chars, last 2 are unk tokens.
    assert_eq!(&tokens[0], "c");
    assert_eq!(&tokens[1], "a");
    assert_eq!(&tokens[2], "f");

    unsafe { talu_sys::talu_tokenize_result_free(result.tokens, result.num_tokens) };
}

/// tokenize "ğŸ‰" returns 4 string tokens (one per byte).
#[test]
fn tokenize_strings_emoji() {
    let ctx = TokenizerTestContext::new();
    let text = "ğŸ‰";
    let result = unsafe {
        talu_sys::talu_tokenizer_tokenize(
            ctx.handle(),
            text.as_bytes().as_ptr(),
            text.len(),
        )
    };
    assert!(result.error_msg.is_null());
    assert_eq!(result.num_tokens, 4);
    unsafe { talu_sys::talu_tokenize_result_free(result.tokens, result.num_tokens) };
}

// ===========================================================================
// Compute offsets with multi-byte input
// ===========================================================================

/// Offsets for "cafÃ©" (5 bytes): 5 contiguous single-byte spans.
#[test]
fn compute_offsets_multibyte() {
    let ctx = TokenizerTestContext::new();
    let text = "cafÃ©";
    let result = unsafe {
        talu_sys::talu_tokenizer_compute_offsets(
            ctx.handle(),
            text.as_bytes().as_ptr(),
            text.len(),
        )
    };
    assert!(result.error_msg.is_null());
    assert_eq!(result.len, 5); // 5 bytes = 5 tokens = 5 offsets

    let offsets = unsafe { std::slice::from_raw_parts(result.offsets, result.len) };
    assert_eq!(offsets[0].start, 0);
    assert_eq!(offsets.last().unwrap().end as usize, text.len());

    // All spans are contiguous.
    for w in offsets.windows(2) {
        assert_eq!(w[0].end, w[1].start, "offsets should be contiguous");
    }

    unsafe { talu_sys::talu_offsets_free(result) };
}

/// Offsets for "ğŸ‰" (4 bytes): 4 spans covering [0,4).
#[test]
fn compute_offsets_emoji() {
    let ctx = TokenizerTestContext::new();
    let text = "ğŸ‰";
    let result = unsafe {
        talu_sys::talu_tokenizer_compute_offsets(
            ctx.handle(),
            text.as_bytes().as_ptr(),
            text.len(),
        )
    };
    assert!(result.error_msg.is_null());
    assert_eq!(result.len, 4);

    let offsets = unsafe { std::slice::from_raw_parts(result.offsets, result.len) };
    assert_eq!(offsets[0].start, 0);
    assert_eq!(offsets[3].end as usize, 4);

    unsafe { talu_sys::talu_offsets_free(result) };
}

/// Offsets for "HiğŸ‰bye" (9 bytes): 9 contiguous spans covering [0,9).
#[test]
fn compute_offsets_mixed_ascii_emoji() {
    let ctx = TokenizerTestContext::new();
    let text = "HiğŸ‰bye";
    let result = unsafe {
        talu_sys::talu_tokenizer_compute_offsets(
            ctx.handle(),
            text.as_bytes().as_ptr(),
            text.len(),
        )
    };
    assert!(result.error_msg.is_null());
    assert_eq!(result.len, text.len());

    let offsets = unsafe { std::slice::from_raw_parts(result.offsets, result.len) };
    assert_eq!(offsets[0].start, 0);
    assert_eq!(offsets.last().unwrap().end as usize, text.len());

    for w in offsets.windows(2) {
        assert_eq!(w[0].end, w[1].start);
    }

    unsafe { talu_sys::talu_offsets_free(result) };
}

// ===========================================================================
// BPE merges + multi-byte input
// ===========================================================================

/// Offsets for CJK text "æ—¥æœ¬" (6 UTF-8 bytes): 6 contiguous single-byte spans.
#[test]
fn compute_offsets_cjk() {
    let ctx = TokenizerTestContext::new();
    let text = "æ—¥æœ¬";
    let result = unsafe {
        talu_sys::talu_tokenizer_compute_offsets(
            ctx.handle(),
            text.as_bytes().as_ptr(),
            text.len(),
        )
    };
    assert!(result.error_msg.is_null());
    assert_eq!(result.len, 6); // 6 bytes = 6 tokens = 6 offsets

    let offsets = unsafe { std::slice::from_raw_parts(result.offsets, result.len) };
    assert_eq!(offsets[0].start, 0);
    assert_eq!(offsets.last().unwrap().end as usize, 6);

    for w in offsets.windows(2) {
        assert_eq!(w[0].end, w[1].start, "offsets should be contiguous");
    }

    unsafe { talu_sys::talu_offsets_free(result) };
}

/// "helloğŸ‰": "hello" merges to one token, emoji bytes â†’ unk.
#[test]
fn merges_with_emoji() {
    let ctx = TokenizerTestContext::with_merges();
    // hello=104, then 4 emoji bytes â†’ 3,3,3,3
    assert_eq!(
        ctx.encode_with("helloğŸ‰", &no_bos()),
        [104, 3, 3, 3, 3]
    );
}

/// "hello world": "hello" merges, spaceâ†’unk, "world" stays chars.
#[test]
fn merges_with_space() {
    let ctx = TokenizerTestContext::with_merges();
    // hello=104, space=3, w=91, o=83, r=86, l=80, d=72
    assert_eq!(
        ctx.encode_with("hello world", &no_bos()),
        [104, 3, 91, 83, 86, 80, 72]
    );
}
