{
  "name": "youtu_vl",
  "model_types": [
    "youtu_vl"
  ],
  "weight_prefixes": [
    "model.layers.{d}.",
    "language_model.model.layers.{d}."
  ],
  "block": [
    {
      "op": "norm",
      "inputs": [
        {
          "tensor": "x"
        },
        {
          "tensor": "input_layernorm.weight"
        }
      ],
      "outputs": [
        "_t0"
      ],
      "eps": 1e-06,
      "name": "input_layernorm"
    },
    {
      "op": "multihead_attention",
      "inputs": [
        {
          "tensor": "_t0"
        }
      ],
      "outputs": [
        "_t1"
      ],
      "qk_norm": false,
      "fused_qkv": false,
      "mla": true,
      "q_lora_rank": 1536,
      "kv_lora_rank": 512,
      "qk_head_dim": 192,
      "qk_rope_head_dim": 64,
      "qk_nope_head_dim": 128,
      "v_head_dim": 128,
      "rope_interleave": true
    },
    {
      "op": "add",
      "inputs": [
        {
          "tensor": "x"
        },
        {
          "tensor": "_t1"
        }
      ],
      "outputs": [
        "_t2"
      ]
    },
    {
      "op": "norm",
      "inputs": [
        {
          "tensor": "_t2"
        },
        {
          "tensor": "post_attention_layernorm.weight"
        }
      ],
      "outputs": [
        "_t3"
      ],
      "eps": 1e-06,
      "name": "post_attention_layernorm"
    },
    {
      "op": "mlp",
      "inputs": [
        {
          "tensor": "_t3"
        }
      ],
      "outputs": [
        "_t4"
      ],
      "activation": "silu",
      "fused_gate_up": false
    },
    {
      "op": "add",
      "inputs": [
        {
          "tensor": "_t2"
        },
        {
          "tensor": "_t4"
        }
      ],
      "outputs": [
        "_t5"
      ]
    }
  ],
  "block_weights": [
    {
      "id": "input_layernorm.weight",
      "module_type": "RMSNorm",
      "layout": "none",
      "dtype": "float32",
      "required": true
    },
    {
      "id": "self_attn.q_a_proj.weight",
      "module_type": "Linear",
      "layout": "linear",
      "dtype": "float32",
      "required": true,
      "expected_shape": [1536, 2560]
    },
    {
      "id": "self_attn.q_a_layernorm.weight",
      "module_type": "RMSNorm",
      "layout": "none",
      "dtype": "float32",
      "required": true
    },
    {
      "id": "self_attn.q_b_proj.weight",
      "module_type": "Linear",
      "layout": "linear",
      "dtype": "float32",
      "required": true,
      "expected_shape": [6144, 1536]
    },
    {
      "id": "self_attn.kv_a_proj_with_mqa.weight",
      "module_type": "Linear",
      "layout": "linear",
      "dtype": "float32",
      "required": true,
      "expected_shape": [576, 2560]
    },
    {
      "id": "self_attn.kv_a_layernorm.weight",
      "module_type": "RMSNorm",
      "layout": "none",
      "dtype": "float32",
      "required": true
    },
    {
      "id": "self_attn.kv_b_proj.weight",
      "module_type": "Linear",
      "layout": "linear",
      "dtype": "float32",
      "required": true,
      "expected_shape": [8192, 512]
    },
    {
      "id": "self_attn.o_proj.weight",
      "module_type": "Linear",
      "layout": "linear",
      "dtype": "float32",
      "required": true,
      "expected_shape": [2560, 4096]
    },
    {
      "id": "post_attention_layernorm.weight",
      "module_type": "RMSNorm",
      "layout": "none",
      "dtype": "float32",
      "required": true
    },
    {
      "id": "mlp.gate_proj.weight",
      "module_type": "Linear",
      "layout": "linear",
      "dtype": "float32",
      "required": true
    },
    {
      "id": "mlp.up_proj.weight",
      "module_type": "Linear",
      "layout": "linear",
      "dtype": "float32",
      "required": true
    },
    {
      "id": "mlp.down_proj.weight",
      "module_type": "Linear",
      "layout": "linear",
      "dtype": "float32",
      "required": true
    }
  ],
  "pre_block": [
    {
      "op": "embedding",
      "inputs": [
        {
          "tensor": "input_ids"
        }
      ],
      "outputs": [
        "_t0"
      ]
    }
  ],
  "post_block": [
    {
      "op": "norm",
      "inputs": [
        {
          "tensor": "_t_last"
        }
      ],
      "outputs": [
        "_t_out"
      ],
      "eps": 1e-06
    }
  ],
  "global_weights": [
    {
      "id": "token_embeddings",
      "candidates": [
        "model.embed_tokens.weight",
        "embed_tokens.weight",
        "transformer.wte.weight",
        "backbone.embedding.weight",
        "language_model.model.embed_tokens.weight"
      ],
      "module_type": "Embedding",
      "layout": "embedding",
      "dtype": "float32",
      "required": true
    },
    {
      "id": "ln_final",
      "candidates": [
        "model.norm.weight",
        "norm.weight",
        "transformer.ln_f.weight",
        "backbone.norm.weight",
        "language_model.model.norm.weight",
        "model.embedding_norm.weight"
      ],
      "module_type": "RMSNorm",
      "layout": "none",
      "dtype": "float32",
      "required": true
    },
    {
      "id": "lm_head",
      "candidates": [
        "lm_head.weight",
        "output.weight",
        "transformer.lm_head.weight",
        "language_model.lm_head.weight"
      ],
      "module_type": "Linear",
      "layout": "linear",
      "dtype": "float32",
      "required": false
    }
  ]
}
