{
  "name": "granite_hybrid",
  "model_types": [
    "granite_hybrid",
    "granitehybrid",
    "granitemoehybrid"
  ],
  "weight_prefixes": [
    "model.layers.{d}.",
    "layers.{d}.",
    "transformer.h.{d}.",
    "backbone.layers.{d}.",
    "language_model.model.layers.{d}."
  ],
  "block_variants": {
    "mamba": {
      "ops": [
        {
          "op": "norm",
          "inputs": [
            {
              "tensor": "x"
            },
            {
              "tensor": "input_layernorm.weight"
            }
          ],
          "outputs": [
            "_t0"
          ],
          "eps": 1e-05,
          "name": "input_layernorm"
        },
        {
          "op": "mamba_mixer",
          "inputs": [
            {
              "tensor": "_t0"
            }
          ],
          "outputs": [
            "_t1"
          ],
          "name": "mixer",
          "d_model": 768,
          "d_state": 128,
          "d_conv": 4,
          "n_heads": 48,
          "d_head": 32,
          "n_groups": 1,
          "expand": 2,
          "d_inner": 1536
        },
        {
          "op": "mul",
          "inputs": [
            {
              "tensor": "_t1"
            },
            {
              "scalar": 0.246
            }
          ],
          "outputs": [
            "_t2"
          ]
        },
        {
          "op": "add",
          "inputs": [
            {
              "tensor": "x"
            },
            {
              "tensor": "_t2"
            }
          ],
          "outputs": [
            "_t3"
          ]
        },
        {
          "op": "norm",
          "inputs": [
            {
              "tensor": "_t3"
            },
            {
              "tensor": "post_attention_layernorm.weight"
            }
          ],
          "outputs": [
            "_t4"
          ],
          "eps": 1e-05,
          "name": "post_attention_layernorm"
        },
        {
          "op": "mlp",
          "inputs": [
            {
              "tensor": "_t4"
            }
          ],
          "outputs": [
            "_t5"
          ],
          "activation": "silu",
          "fused_gate_up": false
        },
        {
          "op": "mul",
          "inputs": [
            {
              "tensor": "_t5"
            },
            {
              "scalar": 0.246
            }
          ],
          "outputs": [
            "_t6"
          ]
        },
        {
          "op": "add",
          "inputs": [
            {
              "tensor": "_t3"
            },
            {
              "tensor": "_t6"
            }
          ],
          "outputs": [
            "_t7"
          ]
        }
      ],
      "weights": [
        {
          "id": "input_layernorm.weight",
          "module_type": "RMSNorm",
          "layout": "none",
          "dtype": "float32",
          "required": true
        },
        {
          "id": "mixer.A_log",
          "module_type": "Mamba2",
          "layout": "none",
          "dtype": "float32",
          "required": true,
          "candidates": [
            "model.layers.{d}.mamba.A_log",
            "model.layers.{d}.mixer.A_log",
            "layers.{d}.mixer.A_log",
            "transformer.h.{d}.mixer.A_log",
            "backbone.layers.{d}.mixer.A_log",
            "language_model.model.layers.{d}.mixer.A_log"
          ]
        },
        {
          "id": "mixer.D",
          "module_type": "Mamba2",
          "layout": "none",
          "dtype": "float32",
          "required": true,
          "candidates": [
            "model.layers.{d}.mamba.D",
            "model.layers.{d}.mixer.D",
            "layers.{d}.mixer.D",
            "transformer.h.{d}.mixer.D",
            "backbone.layers.{d}.mixer.D",
            "language_model.model.layers.{d}.mixer.D"
          ]
        },
        {
          "id": "mixer.dt_bias",
          "module_type": "Mamba2",
          "layout": "none",
          "dtype": "float32",
          "required": true,
          "candidates": [
            "model.layers.{d}.mamba.dt_bias",
            "model.layers.{d}.mixer.dt_bias",
            "layers.{d}.mixer.dt_bias",
            "transformer.h.{d}.mixer.dt_bias",
            "backbone.layers.{d}.mixer.dt_bias",
            "language_model.model.layers.{d}.mixer.dt_bias"
          ]
        },
        {
          "id": "mixer.in_proj.weight",
          "module_type": "Linear",
          "layout": "linear",
          "dtype": "float32",
          "required": true,
          "candidates": [
            "model.layers.{d}.mamba.in_proj.weight",
            "model.layers.{d}.mixer.in_proj.weight",
            "layers.{d}.mixer.in_proj.weight",
            "transformer.h.{d}.mixer.in_proj.weight",
            "backbone.layers.{d}.mixer.in_proj.weight",
            "language_model.model.layers.{d}.mixer.in_proj.weight"
          ]
        },
        {
          "id": "mixer.conv1d.weight",
          "module_type": "Conv1d",
          "layout": "conv1d_depthwise",
          "dtype": "float32",
          "required": true,
          "candidates": [
            "model.layers.{d}.mamba.conv1d.weight",
            "model.layers.{d}.mixer.conv1d.weight",
            "layers.{d}.mixer.conv1d.weight",
            "transformer.h.{d}.mixer.conv1d.weight",
            "backbone.layers.{d}.mixer.conv1d.weight",
            "language_model.model.layers.{d}.mixer.conv1d.weight"
          ]
        },
        {
          "id": "mixer.conv1d.bias",
          "module_type": "Conv1d",
          "layout": "conv1d_depthwise",
          "dtype": "float32",
          "required": true,
          "candidates": [
            "model.layers.{d}.mamba.conv1d.bias",
            "model.layers.{d}.mixer.conv1d.bias",
            "layers.{d}.mixer.conv1d.bias",
            "transformer.h.{d}.mixer.conv1d.bias",
            "backbone.layers.{d}.mixer.conv1d.bias",
            "language_model.model.layers.{d}.mixer.conv1d.bias"
          ]
        },
        {
          "id": "mixer.norm.weight",
          "module_type": "RMSNorm",
          "layout": "none",
          "dtype": "float32",
          "required": true,
          "candidates": [
            "model.layers.{d}.mamba.norm.weight",
            "model.layers.{d}.mixer.norm.weight",
            "layers.{d}.mixer.norm.weight",
            "transformer.h.{d}.mixer.norm.weight",
            "backbone.layers.{d}.mixer.norm.weight",
            "language_model.model.layers.{d}.mixer.norm.weight"
          ]
        },
        {
          "id": "mixer.out_proj.weight",
          "module_type": "Linear",
          "layout": "linear",
          "dtype": "float32",
          "required": true,
          "candidates": [
            "model.layers.{d}.mamba.out_proj.weight",
            "model.layers.{d}.mixer.out_proj.weight",
            "layers.{d}.mixer.out_proj.weight",
            "transformer.h.{d}.mixer.out_proj.weight",
            "backbone.layers.{d}.mixer.out_proj.weight",
            "language_model.model.layers.{d}.mixer.out_proj.weight"
          ]
        },
        {
          "id": "post_attention_layernorm.weight",
          "module_type": "RMSNorm",
          "layout": "none",
          "dtype": "float32",
          "required": true
        },
        {
          "id": "mlp.input_linear.weight",
          "module_type": "Linear",
          "layout": "linear",
          "dtype": "float32",
          "required": true,
          "candidates": [
            "model.layers.{d}.shared_mlp.input_linear.weight",
            "model.layers.{d}.mlp.input_linear.weight",
            "layers.{d}.mlp.input_linear.weight",
            "transformer.h.{d}.mlp.input_linear.weight",
            "backbone.layers.{d}.mlp.input_linear.weight",
            "language_model.model.layers.{d}.mlp.input_linear.weight"
          ]
        },
        {
          "id": "mlp.output_linear.weight",
          "module_type": "Linear",
          "layout": "linear",
          "dtype": "float32",
          "required": true,
          "candidates": [
            "model.layers.{d}.shared_mlp.output_linear.weight",
            "model.layers.{d}.mlp.output_linear.weight",
            "layers.{d}.mlp.output_linear.weight",
            "transformer.h.{d}.mlp.output_linear.weight",
            "backbone.layers.{d}.mlp.output_linear.weight",
            "language_model.model.layers.{d}.mlp.output_linear.weight"
          ]
        }
      ]
    },
    "attention": {
      "ops": [
        {
          "op": "norm",
          "inputs": [
            {
              "tensor": "x"
            },
            {
              "tensor": "input_layernorm.weight"
            }
          ],
          "outputs": [
            "_t0"
          ],
          "eps": 1e-05,
          "name": "input_layernorm"
        },
        {
          "op": "multihead_attention",
          "inputs": [
            {
              "tensor": "_t0"
            }
          ],
          "outputs": [
            "_t1"
          ],
          "qk_norm": false,
          "fused_qkv": false
        },
        {
          "op": "mul",
          "inputs": [
            {
              "tensor": "_t1"
            },
            {
              "scalar": 0.246
            }
          ],
          "outputs": [
            "_t2"
          ]
        },
        {
          "op": "add",
          "inputs": [
            {
              "tensor": "x"
            },
            {
              "tensor": "_t2"
            }
          ],
          "outputs": [
            "_t3"
          ]
        },
        {
          "op": "norm",
          "inputs": [
            {
              "tensor": "_t3"
            },
            {
              "tensor": "post_attention_layernorm.weight"
            }
          ],
          "outputs": [
            "_t4"
          ],
          "eps": 1e-05,
          "name": "post_attention_layernorm"
        },
        {
          "op": "mlp",
          "inputs": [
            {
              "tensor": "_t4"
            }
          ],
          "outputs": [
            "_t5"
          ],
          "activation": "silu",
          "fused_gate_up": false
        },
        {
          "op": "mul",
          "inputs": [
            {
              "tensor": "_t5"
            },
            {
              "scalar": 0.246
            }
          ],
          "outputs": [
            "_t6"
          ]
        },
        {
          "op": "add",
          "inputs": [
            {
              "tensor": "_t3"
            },
            {
              "tensor": "_t6"
            }
          ],
          "outputs": [
            "_t7"
          ]
        }
      ],
      "weights": [
        {
          "id": "input_layernorm.weight",
          "module_type": "RMSNorm",
          "layout": "none",
          "dtype": "float32",
          "required": true
        },
        {
          "id": "mixer.q_proj.weight",
          "module_type": "Linear",
          "layout": "linear",
          "dtype": "float32",
          "required": true,
          "candidates": [
            "model.layers.{d}.self_attn.q_proj.weight",
            "model.layers.{d}.mixer.q_proj.weight",
            "layers.{d}.mixer.q_proj.weight",
            "transformer.h.{d}.mixer.q_proj.weight",
            "backbone.layers.{d}.mixer.q_proj.weight",
            "language_model.model.layers.{d}.mixer.q_proj.weight"
          ]
        },
        {
          "id": "mixer.k_proj.weight",
          "module_type": "Linear",
          "layout": "linear",
          "dtype": "float32",
          "required": true,
          "candidates": [
            "model.layers.{d}.self_attn.k_proj.weight",
            "model.layers.{d}.mixer.k_proj.weight",
            "layers.{d}.mixer.k_proj.weight",
            "transformer.h.{d}.mixer.k_proj.weight",
            "backbone.layers.{d}.mixer.k_proj.weight",
            "language_model.model.layers.{d}.mixer.k_proj.weight"
          ]
        },
        {
          "id": "mixer.v_proj.weight",
          "module_type": "Linear",
          "layout": "linear",
          "dtype": "float32",
          "required": true,
          "candidates": [
            "model.layers.{d}.self_attn.v_proj.weight",
            "model.layers.{d}.mixer.v_proj.weight",
            "layers.{d}.mixer.v_proj.weight",
            "transformer.h.{d}.mixer.v_proj.weight",
            "backbone.layers.{d}.mixer.v_proj.weight",
            "language_model.model.layers.{d}.mixer.v_proj.weight"
          ]
        },
        {
          "id": "mixer.o_proj.weight",
          "module_type": "Linear",
          "layout": "linear",
          "dtype": "float32",
          "required": true,
          "candidates": [
            "model.layers.{d}.self_attn.o_proj.weight",
            "model.layers.{d}.mixer.o_proj.weight",
            "layers.{d}.mixer.o_proj.weight",
            "transformer.h.{d}.mixer.o_proj.weight",
            "backbone.layers.{d}.mixer.o_proj.weight",
            "language_model.model.layers.{d}.mixer.o_proj.weight"
          ]
        },
        {
          "id": "post_attention_layernorm.weight",
          "module_type": "RMSNorm",
          "layout": "none",
          "dtype": "float32",
          "required": true
        },
        {
          "id": "mlp.input_linear.weight",
          "module_type": "Linear",
          "layout": "linear",
          "dtype": "float32",
          "required": true,
          "candidates": [
            "model.layers.{d}.shared_mlp.input_linear.weight",
            "model.layers.{d}.mlp.input_linear.weight",
            "layers.{d}.mlp.input_linear.weight",
            "transformer.h.{d}.mlp.input_linear.weight",
            "backbone.layers.{d}.mlp.input_linear.weight",
            "language_model.model.layers.{d}.mlp.input_linear.weight"
          ]
        },
        {
          "id": "mlp.output_linear.weight",
          "module_type": "Linear",
          "layout": "linear",
          "dtype": "float32",
          "required": true,
          "candidates": [
            "model.layers.{d}.shared_mlp.output_linear.weight",
            "model.layers.{d}.mlp.output_linear.weight",
            "layers.{d}.mlp.output_linear.weight",
            "transformer.h.{d}.mlp.output_linear.weight",
            "backbone.layers.{d}.mlp.output_linear.weight",
            "language_model.model.layers.{d}.mlp.output_linear.weight"
          ]
        }
      ]
    }
  },
  "variant_names": [
    "mamba",
    "attention"
  ],
  "layer_map": [
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    1,
    0,
    0,
    1,
    0,
    0,
    0,
    1,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    1,
    0,
    0,
    0,
    0
  ],
  "pre_block": [
    {
      "op": "embedding",
      "inputs": [
        {
          "tensor": "input_ids"
        }
      ],
      "outputs": [
        "_t0"
      ]
    },
    {
      "op": "mul",
      "inputs": [
        {
          "tensor": "_t0"
        },
        {
          "scalar": 12
        }
      ],
      "outputs": [
        "_t1"
      ]
    }
  ],
  "post_block": [
    {
      "op": "norm",
      "inputs": [
        {
          "tensor": "_t_last"
        }
      ],
      "outputs": [
        "_t_out"
      ],
      "eps": 1e-05
    }
  ],
  "global_weights": [
    {
      "id": "token_embeddings",
      "candidates": [
        "model.embed_tokens.weight",
        "embed_tokens.weight",
        "transformer.wte.weight",
        "backbone.embedding.weight",
        "language_model.model.embed_tokens.weight"
      ],
      "module_type": "Embedding",
      "layout": "embedding",
      "dtype": "float32",
      "required": true
    },
    {
      "id": "ln_final",
      "candidates": [
        "model.norm.weight",
        "norm.weight",
        "transformer.ln_f.weight",
        "backbone.norm.weight",
        "language_model.model.norm.weight",
        "model.embedding_norm.weight"
      ],
      "module_type": "RMSNorm",
      "layout": "none",
      "dtype": "float32",
      "required": true
    },
    {
      "id": "lm_head",
      "candidates": [
        "lm_head.weight",
        "output.weight",
        "transformer.lm_head.weight",
        "language_model.lm_head.weight"
      ],
      "module_type": "Linear",
      "layout": "linear",
      "dtype": "float32",
      "required": false
    }
  ]
}
